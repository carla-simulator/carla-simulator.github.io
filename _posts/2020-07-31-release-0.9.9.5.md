---
layout: post
comments: true
title:  "CARLA 0.9.9.5 release"
subtitle: "Automatized map ingestion, full-road RSS support, accessible OpenDRIVE signals, a new Town in HD and great improvements in other features."
description: "Automatized map ingestion, full-road RSS support, accessible OpenDRIVE signals, a new Town in HD and great improvements in other features."
author: "@sergi-e"
date:   2020-04-22 10:00:00 +0002
image: 'img/carla.jpg'
background: '/img/posts/2019-12-11/Bann001.jpg'
---

Finally, the CARLA team is glad to announce the release of __CARLA 0.9.9.5__! Turn on the AC, and get comfortable. This trip is taking us far.  

CARLA 0.9.9.5 comes with the trunk packed of improvements and new additions to the project. This release comes with notable dedication to sensors, including not only an upgrade for the LIDAR sensor but a brand new raycast LIDAR sensor for more advanced sensor configurations. Furthermore, the rendering pipeline has been improved for a better performance of the RGB cameras in real-time, and the changes made to the sky atmosphere provide for these results to look even better.  

These motif of profound changes and renovation extends to other CARLA modules too. The architecture of Traffic Manager has been revisited to bring the new Traffic Manager 2.0. The integration with Autoware has been improved and eased, and it complements with the anticipated integration with ROS2. The pedestrian module is also undergoing renovation to provide with a more varied and realistic set of walkers that make the simulation feel alive.  

Moreover, there are also new worth-mentioning additions. An integration with OpenStreetMap (still experimental), which allows users to generate CARLA maps based on the road definitions in [OpenStreetMap](https://www.openstreetmap.org), an open-license world map. The new plug-in repository for contributors to share their work, and a remarkable first incorporation to this repository. The carlaviz plug-in, which allows for web browser visualization of the simulation (developed and maintained by [wx9698](https://github.com/wx9698)). We would like to take some extra time to thank all the external contributors that were part of this release. We are grateful for their work, and all their names will figure both in this post, and the release video.  

Last, but not least, we would also like to announce that... __the CARLA AD Leaderboard is finally open to the public!__ If you are working on an AD agent, this may be the best way to test its driving proficiency, and share the results with the rest of the world. Find out more in the [CARLA AD Leaderboard website](https://leaderboard.carla.org/)! 

Here is a recap of all the novelties in CARLA 0.9.9.5!

*   __[LIDAR sensor upgrade](#lidar-sensor-upgrade)__ — More detailed colliders for vehicles and pedestrians, and new parameterization of intensity, noise and points' drop-off.  
*   __[New LIDAR sensor](#new-lidar-sensor)__ — 
*   __[Rendering pipeline optimization](#rendering-pipeline-optimization)__ — Texture streaming and map geometry to get higher FPS values when rendering multiple RGB cameras.  
*   __[OpenStreetMap integration (experimental)](#openstreetmap-integration)__ — A new feature to generate CARLA maps based on an open-license world map.  
*   __[Plug-in repository](#plug-in-repository)__ — A space for CARLA plug-ins and add-ons made and maintained by external contributors.  
*   __[carlaviz plug-in](#carlaviz-plug-in)__ — A new plug-in that allows for visualization of the simulation in a web browser. An external contribution by [wx9698](https://github.com/wx9698).  
*   __[Traffic Manager 2.0](#traffic-manager-2.0)__ — A remodelling of the TM architecture to centralize data, make it accessible, and guarantee the synchronization of calculations and command appliance.  
*   __[ROS2 integration](#ros2-integration)__ — 
*   __[Autoware integration improvements](#autoware-integration-improvements)__ — The new [__carla-autoware repository__](https://github.com/carla-simulator/carla-autoware) provides with an Autoware image, with all the content and configuration needed, and an Autoware agent ready to be used.  
*   __[New RSS features](#new-rss-features)__ — Integration of the new features in ad-rss-lib 4.0.x, which include pedestrians and unstructured scenarios.  
*   __[Pedestrian module expansion](#pedestrian-module-expansion)__ — The first iteration on an extension of the pedestrian module, featuring three new models with much more detailed facial features and clothing.  
*   __[New sky atmosphere](#new-sky-atmosphere)__ — Adjustments in the sun light values and the default values of the RGB camera to fit accordingly.  
*   __[Contributors](#contributors)__ — A space to thank the external contributions since the release of CARLA 0.9.9.  
*   __[Changelog](#changelog)__ — A summary of the fixes featured in this release.  

Let's take a look!

{% include youtube.html id="dvPhN0Gbb0w" %}

{% include release_button.html release_tag="0.9.9" %}

<br>

## LIDAR sensor upgrade
---

The LIDAR now includes some additional features that make the sensor simulation more realistic.  

*   __Intensity__ *(`raw_data` output attribute)*  
The output data retrieved by the LIDAR sensor is now a 4D array of points. The fourth dimension contains an approximation to the intensity of the received ray. The intensity is reduced over time as the ray travels according to the formula:
`intensity / original_intensity = e * (-attenuation_coef * distance)`
The coefficient of attenuation may depend on the sensor's wavelenght and in the conditions of the atmosphere and it can be modified with the LIDAR attribute:
`atmosphere_attenuation_rate`.  
*   __Drop-off__ *(`dropoff_general_rate`, `dropoff_intensity_limit`, `dropoff_zero_intensity` LIDAR attributes)*  
In real sensors, some cloud points can be loss due to multiple reasons like perturbations on the atmosphere or sensor errors. We simulate this with two different models: 
	*   __General drop-off__ — Proportion of points that we drop-off randomly. This is done before the tracing, meaning the points being dropped are not calculated and therefore improves the performance. If dropoff_general_rate = 0.5`, half of the points will be dropped.
	*   __Instensity-based drop-off__ — For each point detected, and extra drop-off is performed with a probability based in the computed intensity. This probability is determined by parameters. `dropoff_zero_intensity` is the probability of points with zero intensity to be dropped. `dropoff_intensity_limit` is a threshold intensity above which no points will be dropped. The probability of a point within the range to be dropped is a linear proportion based on these two parameters.  
*   __Noise__ *(`noise_stddev` LIDAR attribute)*  
This attribute measures unexpected deviations that appear when in real-life sensors. If the noise is positive, the location of each point will be randomly perturbed along the vector of the ray detecting it.

More detailed colliders have also been developed for vehicles and pedestrians, so now the point cloud retrieved by the sensor delineates the elements in the scene much more accurately.  

![lidar_sensor](/img/posts/2020-31-07/lidar_old.gif){:class="img-fluid"}

## New LIDAR sensor
---


![raycast_lidar_sensor](/img/posts/2020-31-07/lidar_raycast.gif){:class="img-fluid"}


## Rendering pipeline optimization
---

In order to improve the performance of the RGB camera sensors, some adjustments in the rendering pipeline were needed. These cameras retrieve a texture, generated in the GPU, which is sent to the CPU when required. The textures contained in the GPU atlas are separated, and the sensors stream their corresponding texture all at the same time. Thus, the amount of times the GPU has to pause calculations to send textures is less, and these pauses do not happen mid-cycle. Besides that, the team is working on improving the geometry of the maps, so that cameras require less time to paint the scene.  

Thanks to these improvements, the simulation can feature multiple cameras without huge FPS drop-offs. For instance, here is CARLA running with four RGB cameras rendering in real-time at X FPS.  

![rendering_pipeline](/img/posts/2020-31-07/rendering_pipeline.gif){:class="img-fluid"}

## OpenStreetMap integration
---

OpenStreetMap is an open license map of the world developed by contributors. Sections of these map can be exported to an XML file in `.osm` format. CARLA can convert this file to OpenDRIVE format and ingest it as any other OpenDRIVE map using the [OpenDRIVE Standalone Mode](https://carla.readthedocs.io/en/latest/adv_opendrive/). The process is detailed in the [documentation](!!link), but here is a summary.  

*   __1. Obtain a map with OpenStreetMap__ — Go to [OpenStreetMap](https://www.openstreetmap.org) to generate an XML file containing the map information.  

*   __2. Convert to OpenDRIVE format__ CARLA can read a XML file generated with OpenStreetMap, and easily convert it to OpenDRIVE format. In order to do the conversion from `.osm` to `.xodr` format, two classes have been added to the Python API.  

*   __[carla.Osm2Odr](python_api.md#carla.osm2odr)__ – The class that does the conversion. It takes the content of the `.osm` parsed as strind, and returns a string containing the resulting `.xodr`.  
	*   `osm_file` — The content of the initial `.osm` file parsed as string.  
	*   `settings` — A [carla.Osm2OdrSettings](python_api.md#carla.Osm2OdrSettings) object containing the settings to parameterize the conversion.  
*   __[carla.Osm2OdrSettings](python_api.md#carla.Osm2OdrSettings)__ – Helper class that contains different parameters used during the conversion.  
	*   `use_offsets` *(default False)* — Determines whereas the map should be generated with an offset, thus moving the origin from the center according to that offset.  
	*   `offset_x` *(default 0.0)* — Offset in the X axis.  
	*   `offset_y` *(default 0.0)* — Offset in the Y axis.  
	*   `default_lane_width` *(default 4.0)* — Determines the width that lanes should have in the resulting XODR file.  
	*   `elevation_layer_height` *(default 0.0)* — Determines the height separating elements in different layers, used for overlapping elements. Read more on [layers](https://wiki.openstreetmap.org/wiki/Key:layer).  


*   __3. Import into CARLA__ – The OpenDRIVE file can be automatically ingested in CARLA using the [OpenDRIVE Standalone Mode](#adv_opendrive.md), either using a customized script of the `config.py` example provided in CARLA.  

<div class="alert alert-warning">
  <small><i><strong>Warning!</strong> This feature is still in experimental phase.</i></small>
</div>

## Plug-in repository
---

A new repository has been created purposely for external contributions. The purpose is for contributors to have their own space to create and maintain plug-ins and add-ons for CARLA.  

Take the chance, and share your work in the [plug-in repository](https://github.com/carla-simulator/carla-plugins)! CARLA is grateful to all the [contributors](#contributors) who dedicate their time to make the project grow, and this is the perfect space to visibilize their hard work.  


## carlaviz plug-in
---

The carlaviz plug-in is used to visualize the simulation in a web browser. This plug-in was created by the contributor [wx9698](https://github.com/wx9698), and it is maintained in the [carlaviz repository](https://github.com/carla-simulator/carlaviz/tree/edfa95dd974be96039601a1137842c3813bdb227). When using it, a window with some basic representation of the scene is created. Actors are updated on-the-fly, sensor data can be retrieved, and additional text, lines and polylines can be drawn in the scene. There is detailed [carlaviz documentation](https://carla.readthedocs.io/en/latest/plugins_carlaviz/) already available, but here is a brief summary on how to run the plug-in and the output it provides.  

__1. Run CARLA.__  

__2. Run carlaviz.__ In another terminal run the following command according to the Docker image that has been downloaded. Edit the comand to match the Docker image being used.  

Change `<name_of_Docker_image>` for the name of the image previously downloaded, e.g. `carlasim/carlaviz:latest` or `mjxu96/carlaviz:0.9.9`.  

```sh
docker run -it --network="host" -e CARLAVIZ_HOST_IP=localhost -e CARLA_SERVER_IP=localhost -e CARLA_SERVER_PORT=2000 <name_of_Docker_image>
```
__3. Open the localhost__ Open your web browser and go to `http://127.0.0.1:8080/`. carlaviz runs by default in port `8080`.  

Once the plug-in is operative, it can be used to visualize the simulation, the actors that live in it, and the data the sensors retrieve. The plug-in shows a visualization window on the right, were the scene is updated in real-time, and a sidebar on the left with a list of items to be shown. Some of these items will appear in the visualization window, others (mainly sensor and game data) appear just above the items list. The result will look similar to the following.  

![carlaviz](/img/posts/2020-31-07/carlaviz.gif){:class="img-fluid"}

## Traffic Manager 2.0
---

For this iteration, the inner structure and logic of the Traffic Manager module has been revamped. These changes are explained in detail in the [Traffic Manager documentation](https://carla.readthedocs.io/en/latest/adv_traffic_manager/). Here is a brief summary of the principles that set the ground for the new architecture.  

*   __Data query centralization.__ The most impactful component in the new Traffic Manager 2.0 logic is the ALSM. It takes care of all the server calls necessary to get the current state of the simulation, and stores everything that will be needed further on: lists of vehicles and walkers, their position and velocity, static attributes such as bounding boxes, etc. Everything is queried by the ALSM and cached so that computational cost is reduced, and redundant API calls are avoided. Additionally, these will be used by different components, such as paths, and vehicle tracking is externalized in other components, so that there is no information dependency.  

*   __Per-vehicle loop structure.__ Previously, in Traffic Manager, the calculations were divided in global stages. They were self-contained, which made it difficult to save up computational costs. Later in the pipeline, stages had no knowledge of the vehicle calculations done previously. Changing these global stages to a per-vehicle structure makes it easier to implement features such as parallellization, as the processing of stages can be triggered externally.  

*   __Loop control.__ This per-vehicle structure brings another issue: synchronization between vehicles has to be guaranteed. For said reason, a component is created to control the loop of the vehicle calculations. This controller creates synchronization barriers that force each vehicle to wait for the rest to finish their calculations. Once all the vehicles are done, the following stage is triggered. That ensures that all the vehicle calculations are done in sync, and there is no frame delay between the processing cycle and the commands being applied.  

## ROS2 integration
---

## Autoware integration improvements
---

The [Autoware-CARLA bridge](https://github.com/Autoware-AI/simulation/tree/master/carla_simulator_bridge) is part of the Autoware repository. However, the bridge does nothing by itself, as it is only in charge of communication between Autoware and ROS. The integration needs for some elements to run.  

*   The [Autoware repository](https://github.com/Autoware-AI/autoware.ai). This already contains the Autoware-CARLA bridge.  

*   The [Autoware content repository](https://bitbucket.org/carla-simulator/autoware-contents/src/master/). This contains the CARLA maps ready to be used in Autoware. As Autoware does not use XODR maps, these have to be translated to point cloud and vector maps, with some additional configuration files.  

*   An Autoware agent which describes the ego vehicle and the sensor setting.  

*   Some basic launch files for the agent, and configuration files describing the topics being published and their name translation between Autoware and ROS.  

*   A CARLA server with the CARLA-ROS bridge enabled.  

In order to ease the usage of these integration, the [__carla-autoware repository__](https://github.com/carla-simulator/carla-autoware) provides with an Autoware image, and an Autoware agent ready to be used. In order to run the example Autoware agent, follow these steps.  

__1. Clone the [carla-autoware repository](https://github.com/carla-simulator/carla-autoware).__  This repository already contains all thar is needed: an Autoware image, the Autoware content, and a basic Autoware agent that can be edited.  
```sh
git clone --recurse-submodules https://github.com/carla-simulator/carla-autoware
```

__2. Build the repository.__
```sh
cd carla-autoware
./build.sh
```

__3. Run a CARLA server with ROS bridge enabled.__ Alternatively, a CARLA docker image can be used. Find out more [here](https://carla.readthedocs.io/en/latest/build_docker/).

__4. Run the Autoware image in the repository.__
```sh
./run.sh
```

__5. Launch the agent.__ 
```sh
roslaunch carla_autoware_agent carla_autoware_agent.launch
```

Here is an example of the Autoware agent running, using [rviz](http://wiki.ros.org/rviz) for visualization.  

![autoware_agent](/img/posts/2020-31-07/autoware_agent.gif){:class="img-fluid"}

## New RSS features
---

The RSS sensor in CARLA now has full support for [ad-rss-lib 4.0.x](https://github.com/intel/ad-rss-lib/releases), which includes two main features.  

*   __Unstructured roads__ — In summary, scenarios were vehicles move in a route where no specific lanes are defined. 

![rss_unstructured](/img/posts/2020-31-07/rss_unstructured.gif){:class="img-fluid"}

*   __Pedestrians__ — Moving in both structured and structured scenarios.  

![rss_pedestrians](/img/posts/2020-31-07/rss_pedestrians.gif){:class="img-fluid"}

Find out more about these features either in the original [RSS paper](https://arxiv.org/abs/1708.06374) or reading the rss-lib documentation on [unstructured scenes](https://intel.github.io/ad-rss-lib/ad_rss/UnstructuredScenes/), and [behavior model for pedestrians](https://intel.github.io/ad-rss-lib/ad_rss/UnstructuredScenes/#behavior-modeltrajectory-calculation).  


## Pedestrian module expansion
---

The CARLA team is currently working on a major extension of the pedestrian module. Our goal is not only to provide a more diverse set of walkers, which is important to recreate reality accurately. The focus is also in the details: more detailed models, attention to facial features, new shaders and materials for their skin, hair, eyes... In summary, we want to make them, and therefore the simulation, feel alive.  

So far, there are three new models in the blueprint library, a sneak peek on what is to come. Let's see how things are going!  

![pedestrians_models](/img/posts/2020-31-07/pedestrians_models02.png){:class="img-fluid"}

![pedestrians_faces](/img/posts/2020-31-07/pedestrians_faces.png){:class="img-fluid"}


## New sky atmosphere
---

The sun light of the simulation has been adjusted to values closer to reality (at least, what can achieved in Unreal Engine). Due to these changes, the default values of the RGB camera sensor have been balanced accordingly, so now its parameterization is also more realistic. 

An additional configuration must be taken into account though. In order to save the user having to change the RGB camera attributes depending on the sun altitude, the light values of the scene are altered when the sun position changes, so that a RGB camera with default values always renders appealing images.  

![sky_atmosphere](/img/posts/2020-31-07/sky_atmosphere.jpg){:class="img-fluid"}

## Contributors
---

The CARLA team is grateful for external contributions that help the project grow even more. For said reason, we would like to dedicate this space to all of those whose contributions were merged in any of the project's GitHub repositories. 

- [arkadiy-telegin](https://github.com/arkadiy-telegin)  
- [Bitfroest](https://github.com/Bitfroest)  
- [dennisrmaier](https://github.com/dennisrmaier)  
- [Diego-ort](https://github.com/Diego-ort)  
- [Hakhyun-Kim](https://github.com/Hakhyun-Kim)  
- [hofbi](https://github.com/hofbi)  
- [ItsTimmy](https://github.com/ItsTimmy)  
- [johschmitz](https://github.com/johschmitz)  
- [kbu9299](https://github.com/kbu9299)  
- [ll7](https://github.com/ll7)  
- [patmalcolm91](https://github.com/patmalcolm91)  
- [pedroburito](https://github.com/pedroburito)  
- [PhDittmann](https://github.com/PhDittmann)  
- [squizz617](https://github.com/squizz617)  
- [umateusz](https://github.com/umateusz)  
- [Vaan5](https://github.com/Vaan5)  
- [wx9698](https://github.com/wx9698)  

## Changelog
---

  + Upgraded to AD RSS v4.0.1 supporting unstructured scenes and pedestrians.
  + Fixed a bug where `get_traffic_light` would always return `None`.
  + Changed frozen behavior for traffic lights. It now affects to all traffic lights at the same time.
  + Added API function `freeze_all_traffic_lights` and `reset_group`.
  + Fixed recorder determinism problems.
  + Added Light ids.
  + Added vehicle light and street light data to recorder.
  + Added API function `add_angular_impulse()` to add angular impulse to any actor.
  + Fixed rain drop spawn issues when spawning camera sensors.
  + Fixed assets import pipeline.
  + Fixed Update.sh from failing when the root folder contains a space on it.
  + Fixed colors of lane markings when importing a map, they were reversed (white and yellow).
  + Fixed missing include directive in file **WheelPhysicsControl.h**.
  + Fixed gravity measurement bug from IMU sensor.
  + All sensors are now multi-stream, that means that the same sensor can be listened from different clients.
  + Fixed point cloud of LiDAR. Now the points are given correctly in the sensor's coordinate system.
  + Exposed matrix form of transformation to the client and Python API.
  + Added make command to download contributions as plugins (`make plugins`).
  + Added PythonAPI command to set multiple car light states at once.
  + Added PythonAPI `carla.world.get_vehicles_light_states` to get all the car light states at once.
  + OpenDRIVE ingestion bugfixes.
  + Added a warning if the user tries to use the SpringArm exactly in the 'z' axis of the attached actor.
  + Improved the LiDAR and Radar sensors with a parallel implentation of the raycasting.
  + Added an approximation of the intensity of each point of the cloud in the LiDAR sensor.
  + Added Dynamic Vision Sensor (DVS) camera based on ESIM simulation http://rpg.ifi.uzh.ch/esim.html.
  + Improved LiDAR and radar to better match the shape of the vehicles.
  + Added support for additional TraCI clients in Sumo co-simulation.
  + Added API functions `get_right_vector` and `get_up_vector`.
  + Added default values and a warning message for lanes missing the width parameter in OpenDRIVE.
  + Added parameter to enable/disable pedestrian navigation in standalone mode.
  + Improved mesh split in standalone mode.
  + Fixed delay in the tcp communication from server to client, improving performance in synchronous mode in linux systems.
  + Fixed large RAM usage when loading polinomial geometry from OpenDRIVE.
  + Fixed collision issues when debug draw(debug.draw_line) is called.
  + Fixed Gyroscope sensor to properly give angular velocity readings in local frame.
  + Added Renderdoc plugin to the Unreal project.
  + Added configurable noise to Lidar sensor.
  + Replace deprectated `platform.dist()` with recommended `distro.linux_distribution()`.
