---
layout: post
comments: true
title:  "CARLA 0.9.9.5 release"
subtitle: "Automatized map ingestion, full-road RSS support, accessible OpenDRIVE signals, a new Town in HD and great improvements in other features."
description: "Automatized map ingestion, full-road RSS support, accessible OpenDRIVE signals, a new Town in HD and great improvements in other features."
author: "@sergi-e"
date:   2020-04-22 10:00:00 +0002
image: 'img/carla.jpg'
background: '/img/posts/2019-12-11/Bann001.jpg'
---

The CARLA team is thrilled to release __CARLA 0.9.9__! Buckle up, because it comes ready to drift!  

The development has been brief but intense. There are plenty of announces to make, some of great importance. Especially remarkable is the new map ingestion proccess. This has been simplified for users to use their maps in CARLA out-of-the-box. Traffic lights and traffic signs will nevermore require users' manual intervention. Instead, CARLA can now automatically ingest these traffic landmarks directly from OpenDRIVE.  

Moreover, previous features and modules are growing strong. The RSS sensor, SUMO co-simulation, OpenDRIVE standalone mode, and Traffic Manager all have new features, and improved performance. There is even a new integration with PTV-Vissim. The documentation keeps up to date with all of these improvements, and features new tutorials. Last but not least, there is a new map in CARLA. __It is opening day in Town10HD!__ 

Here is a summary of every feature included in CARLA 0.9.9!

—
*   __[LIDAR sensor upgrade](#lidar-sensor-upgrade)__ — 
*   __[New LIDAR sensor](#new-lidar-sensor)__ — 
*   __[Rendering pipeline optimization](#rendering-pipeline-optimization)__ — 
*   __[OpenStreetMap integration.](#openstreetmap-integration)__ — 
*   __[Plug-in repository](#plug-in-repository)__ — 
*   __[carlaviz plug-in](#carlaviz-plug-in)__ — 
*   __[Traffic Manager 2.0](#traffic-manager-2.0)__ — 
*   __[ROS2 integration](#ros2-integration)__ — 
*   __[Autoware integration improvements](#autoware-integration-improvements)__ — 
*   __[New RSS features](#new-rss-features)__ — 
*   __[Pedestrian module expansion](#pedestrian-module-expansion)__ — 
*   __[New Sky atmosphere](#new-sky-atmosphere)__ — 
*   __[Contributors](#contributors)__ — 
*   __[Changelog](#changelog)__ — 
*   __[CARLA Leaderboard announcement](#carla-leaderboard-announcement)__ — 

Let's take a look!

{% include youtube.html id="dvPhN0Gbb0w" %}

{% include release_button.html release_tag="0.9.9" %}

<br>

## LIDAR sensor upgrade
---

The LIDAR sensor now includes some additional attributes that make it feature a more realistic behavior.  

*   __Noise__ *(`noise_stddev` LIDAR attribute)*  
This attribute simulates unexpected deviations that appear when taking real-life measurements. If the noise is positive, the location of each point will be randomly perturbed along the vector of the ray detecting it.  

*   __Intensity__ *(`raw_data` output attribute)*  
The output data retrieved by the LIDAR sensor is now a 4D array of points. The fourth dimension contains the intensity of the ray when the point detected was reached. The intensity is reduced over time as the ray travels according to the formula:  
`intensity / original_intensity = e * (-attenuation_coef * distance)`  
The coefficient of attenuation can be modified with the LIDAR attribute:  
`atmosphere_attenuation_rate`.  

*   __Drop-off__ *(`dropoff_general_rate`, `dropoff_intensity_limit`, `dropoff_zero_intensity` LIDAR attributes)*  
Cloud points can be dropped cloud points. This is done in two different ways. In a general way, 
	*   __General drop-off__ — A random drop-off done before the tracing, meaning the points being dropped are not calculated. This drop-off is expressed with a proportion. If `dropoff_general_rate = 0.5`, half the points will be dropped.  
	*   __Instensity-based drop-off__ — For each point detected, a calculation is done. If the 
we can randomly drop points with a probability given by `dropoff_general_rate`. In this case, the drop off of points is done before tracing the ray cast so adjust this parameter can increase our performance. If that parameter is set to zero it will be ignored.  
The second way to regulate the drop off of points is in a rate proportional to the intensity. This drop off rate will be proportional to the intensity from zero at `dropoff_intensity_limit` to `dropoff_zero_intensity` at zero intensity.


## New LIDAR sensor
---

## Rendering pipeline optimization
---

In order to improve the performance of the RGB camera sensors, some adjustments in the rendering pipeline were needed. These cameras retrieve a texture, generated in the GPU, which is sent to the CPU when required. These textures are now coded in an atlas and sent all-together at the end of the rendering pipeline. Thus, the amount of times the GPU has to pause calculations to send textures is less, and these pauses do not happen mid-cycle. Additionally, some changes in the map geometry have been made, so that cameras require less time to paint the scene. 

Thanks to these improvements, the simulation can feature multiple cameras without huge FPS drop-offs. For instance, here is CARLA running with four RGB cameras rendering in real-time at X FPS.  

> Gif. 

## OpenStreetMap integration
---

OpenStreetMap is an open license map of the world developed by contributors. Sections of these map can be exported to an XML file in `.osm` format. CARLA can convert this file to OpenDRIVE format and ingest it as any other OpenDRIVE map using the [OpenDRIVE Standalone Mode](https://carla.readthedocs.io/en/latest/adv_opendrive/). The process is detailed in the [documentation](!!link), but here is a summary.  

*   __1  + Obtain a map with OpenStreetMap__ — Go to [OpenStreetMap](https://www.openstreetmap.org) to generate an XML file containing the map information.  

*   __2  + Convert to OpenDRIVE format__ CARLA can read a XML file generated with OpenStreetMap, and easily convert it to OpenDRIVE format.  

*   __3  + Import into CARLA__ – The OpenDRIVE file can be automatically ingested in CARLA using the [OpenDRIVE Standalone Mode](#adv_opendrive.md), either using a customized script of the `config.py` example provided in CARLA.  

In order to do the conversion from `.osm` to `.xodr` format, two classes have been added to the Python API.  

*   __[carla.Osm2Odr](python_api.md#carla.osm2odr)__ – The class that does the conversion. It takes the content of the `.osm` parsed as strind, and returns a string containing the resulting `.xodr`.  
	*   `osm_file` — The content of the initial `.osm` file parsed as string.  
	*   `settings` — A [carla.Osm2OdrSettings](python_api.md#carla.Osm2OdrSettings) object containing the settings to parameterize the conversion.  
*   __[carla.Osm2OdrSettings](python_api.md#carla.Osm2OdrSettings)__ – Helper class that contains different parameters used during the conversion.  
	*   `use_offsets` *(default False)* — Determines whereas the map should be generated with an offset, thus moving the origin from the center according to that offset.  
	*   `offset_x` *(default 0.0)* — Offset in the X axis.  
	*   `offset_y` *(default 0.0)* — Offset in the Y axis.  
	*   `default_lane_width` *(default 4.0)* — Determines the width that lanes should have in the resulting XODR file.  
	*   `elevation_layer_height` *(default 0.0)* — Determines the height separating elements in different layers, used for overlapping elements. Read more on [layers](https://wiki.openstreetmap.org/wiki/Key:layer).  

## Plug-in repository
---

## Traffic Manager 2.0
---

For this iteration, the inner structure and logic of the Traffic Manager module has been remodeled. The details these changes are explained in the [Traffic Manager documentation](https://carla.readthedocs.io/en/latest/adv_traffic_manager/). Here is a brief summary on the principles that set the ground for the new architecture.  

*   __Data query centralization.__ The first and foremost component in the new TM 2.0 logic is the ALSM. This component takes care of all the server calls necessary to get the current state of the simulation. Everything that will be needed further on: lists of vehicles and walkers, their position and velocity, static attributes such as bounding boxes, etc. Everything is queried by the ALSM and stored in cache so that cost is reduced, and redundant API calls avoided. Additionally, later on information that will be used by different components, such as paths, or vehicle tracking is externalized in other components, so that there is no information dependency.  

*   __Per-vehicle loop structure.__ Previously in Traffic Manager, the calculations were divided in global stages. Calculations were self-contained, which made it difficult to save up costs. Going downstream, later stages had no knowledge of the vehicle calculations done previously. Changing this global stages to a per-vehicle structure makes it easier to implement things such as parallellization, as the processing of stages can be triggered externally.  

*   __Loop control.__ This per-vehicle structure brings another issue: synchronization between vehicles has to be guaranteed. For said reason, a component is created to control the loop of the vehicle calculations. This controller creates synchronization barriers that force each vehicle to wait for the rest to finish their calculations. Once all the vehicles are done, the folowing stage is triggered. That ensures that there all the vehicle calculations are done in sync, and there is no frame delay between the processing cycle and the commands being applied.  

## ROS2 integration
---

## Autoware integration improvements
---

The [Autoware-CARLA bridge](https://github.com/Autoware-AI/simulation/tree/master/carla_simulator_bridge) is part of the Autoware repository. However, the bridge does nothing by itself, as it is only in charge of communication between Autoware and ROS. The integration needs for some elements to run.  

*   The [Autoware repository](https://github.com/Autoware-AI/autoware.ai). This already contains the Autoware-CARLA bridge.  

*   The [Autoware content repository](https://bitbucket.org/carla-simulator/autoware-contents/src/master/). This contains the CARLA maps ready to be used in Autoware. As Autoware does not use XODR maps, these have to be translated to point cloud and vector maps, with some additional configuration files.  

*   An Autoware agent which describes the ego vehicle and the sensor setting.  

*   Some basic launch files for the agent, and configuration files describing the topics being published and their name translation between Autoware and ROS.  

*   A CARLA server with the CARLA-ROS bridge enabled.  

In order to ease the usage of these integration, the [__carla-autoware repository__](https://github.com/carla-simulator/carla-autoware) provides with an Autoware imagen, and an Autoware agent ready to be used. In order to run the example Autoware agent, follow these steps.  

__1. Clone the [carla-autoware repository](https://github.com/carla-simulator/carla-autoware).__  This repository already contains all thar is needed: an Autoware image, the Autoware content, and a basic Autoware agent that can be edited.  
```sh
git clone --recurse-submodules https://github.com/carla-simulator/carla-autoware
```

__2. Build the repository.__
```sh
cd carla-autoware
./build.sh
```

__3. Run a CARLA server with ROS bridge enabled.__ Alternatively, a CARLA docker image can be used. Find out more [here](https://carla.readthedocs.io/en/latest/build_docker/).

__4. Run the Autoware image in the repository.__
```sh
./run.sh
```

__5. Launch the agent.__ 
```sh
roslaunch carla_autoware_agent carla_autoware_agent.launch
```

Additionally, the [carlaviz plug-in](#carlaviz-plug-in) can be used for visualization.  

> Añadir gif donde se vea la integración. 


## New RSS features
---

The RSS sensor in CARLA now has full support for [ad-rss-lib 4.0.x](https://github.com/intel/ad-rss-lib/releases), which includes two main features.  

*   __Unstructured roads__ — In summary, scenarios were vehicles move in a route where no specific lanes are defined. 
*   __Pedestrians__ — Moving in both structured and structured scenarios.  

Find out more about these features either in the original [RSS paper](https://arxiv.org/abs/1708.06374) or reading the rss-lib documentation on [unstructured scenes](https://intel.github.io/ad-rss-lib/ad_rss/UnstructuredScenes/), and [behavior model for pedestrians](https://intel.github.io/ad-rss-lib/ad_rss/UnstructuredScenes/#behavior-modeltrajectory-calculation).  



## Pedestrian module expansion
---

Nuevos modelos (afros y old)
Si quieres di que tienne mas poligonos, en la cara y que hemos hecho cambios en el cuerpo
pero que han de haber mas variaciones y nuevos modelos en un futuro
y no se me ocurre mas, de nuevos shaders y materiales, de piel , pelo y ojos,  igual hablaria , pero sin entrar mucho


## New Sky atmosphere
---

pero en la release anterior ya era nueva atmosphera. Lo que hemos tocado ahora son los valores, que antes las luces eran muy muy flojas y se compensaba poniendo una configuracion de camara super sensible a la luz, como si estuvieras haciendo fotos a oscuras, y de esa manera compensaba y parecía de día. Eso nos limitaba mucho. Ahora todo está reajustado
antes era parecer realistas a ojo, teneiendo unos valores que creiamos que parece realista, y ahora estamos poniendo valores reales y el realismo es lo que unreal pueda ofrecer
lo que no hemos hecho es cambiar la configuracion de la camara al hacerse de noche. Tenemos una cámara con la config para hacer fotos de dia soleado. Si se pasa a modo noche, en teoria se tendría que cambiar manualmente la configuracion de la camara. Porque la tenemos en manual. Pero eso puede ser muy engorro para el usuario, que si mueve el sol se verá todo oscuro porque tambien ha de cambiar la camara. Por lo que hemos hecho es ajustar los valores de intensidad de farolas y demás, para que se vea modo noche pero bonito pero sin tener que cambiar la config de la camra
La cámara esta preparada para hacer foto chulas de día. Si pasamos a noche, con la misma confuguracion debería verse todo oscuro aunque haya farola. Por lo que lo compensamos poniendo valores super altos de luces de calle. Por lo que valores reales sólo es de día
Hemos conseguido poner valores entremedio para que se vea bien sin necesidad de tocar la cámara. Aunque hay alguna cosilla por ahí que no acaba de ser 100% realista pero lo damos por bueno
Te lo comento porque entonces no podemos vender que todos los valores son valores de la vida real, porque luz de farolas y coches están mucho más alto para compensar que no cambias la apertura de la cámara

-Entonces lo que se hace es, cuando cambia la iluminación (altura del sol vaya), se cambian las luces acorde para que funcionen con los default values de la cámara RGB?
-Sí, en lugar de hacer al usuario que tenga que ajustar su cámara a la luz que hay en ese momento (como se haría en la vida real), le hemos puesto una configuración que sirve de día y compensado la falta de luz de noche con luces super fuertes y se ve bien entonces


## Contributors
---

The CARLA team is grateful for external contributions that help the project grow even more. For said reason, we would like to dedicate this space to all of those whose contributions were merged in any of the project's GitHub repositories. 

- [arkadiy-telegin](https://github.com/arkadiy-telegin)  
- [Bitfroest](https://github.com/Bitfroest)  
- [dennisrmaier](https://github.com/dennisrmaier)  
- [Diego-ort](https://github.com/Diego-ort)  
- [Hakhyun-Kim](https://github.com/Hakhyun-Kim)  
- [hofbi](https://github.com/hofbi)  
- [ItsTimmy](https://github.com/ItsTimmy)  
- [johschmitz](https://github.com/johschmitz)  
- [kbu9299](https://github.com/kbu9299)  
- [ll7](https://github.com/ll7)  
- [patmalcolm91](https://github.com/patmalcolm91)  
- [pedroburito](https://github.com/pedroburito)  
- [PhDittmann](https://github.com/PhDittmann)  
- [squizz617](https://github.com/squizz617)  
- [umateusz](https://github.com/umateusz)  
- [Vaan5](https://github.com/Vaan5)  
- [wx9698](https://github.com/wx9698)  

## Changelog
---

- __Section__ 
  + Upgraded to AD RSS v4.0.1 supporting unstructured scenes and pedestrians
  + Fixed a bug where `get_traffic_light` would always return `None`
  + Changed frozen behavior for traffic lights. It now affects to all traffic lights at the same time
  + Added API function `freeze_all_traffic_lights` and `reset_group`
  + Fixed recorder determinism problems
  + Added Light ids
  + Added vehicle light and street light data to recorder
  + Added API function `add_angular_impulse()` to add angular impulse to any actor
  + Fixed rain drop spawn issues when spawning camera sensors
  + Fixed assets import pipeline
  + Fixed Update.sh from failing when the root folder contains a space on it
  + Fixed colors of lane markings when importing a map, they were reversed (white and yellow)
  + Fixed missing include directive in file **WheelPhysicsControl.h**
  + Fixed gravity measurement bug from IMU sensor
  + All sensors are now multi-stream, that means that the same sensor can be listened from different clients
  + Fixed point cloud of LiDAR. Now the points are given correctly in the sensor's coordinate system.
  + Exposed matrix form of transformation to the client and Python API.
  + Added make command to download contributions as plugins (`make plugins`)
  + Added PythonAPI command to set multiple car light states at once
  + Added PythonAPI `carla.world.get_vehicles_light_states` to get all the car light states at once
  + OpenDRIVE ingestion bugfixes
  + Added a warning if the user tries to use the SpringArm exactly in the 'z' axis of the attached actor
  + Improved the LiDAR and Radar sensors with a parallel implentation of the raycasting
  + Added an approximation of the intensity of each point of the cloud in the LiDAR sensor
  + Added Dynamic Vision Sensor (DVS) camera based on ESIM simulation http://rpg.ifi.uzh.ch/esim.html
  + Improved LiDAR and radar to better match the shape of the vehicles
  + Added support for additional TraCI clients in Sumo co-simulation
  + Added API functions `get_right_vector` and `get_up_vector`
  + Added default values and a warning message for lanes missing the width parameter in OpenDRIVE
  + Added parameter to enable/disable pedestrian navigation in standalone mode
  + Improved mesh split in standalone mode
  + Fixed delay in the tcp communication from server to client, improving performance in synchronous mode in linux systems
  + Fixed large RAM usage when loading polinomial geometry from OpenDRIVE
  + Fixed collision issues when debug draw(debug.draw_line) is called
  + Fixed Gyroscope sensor to properly give angular velocity readings in local frame
  + Added Renderdoc plugin to the Unreal project
  + Added configurable noise to Lidar sensor
  + Replace deprectated `platform.dist()` with recommended `distro.linux_distribution()`

## CARLA Leaderboard announcement
---





![rss_junctions](/img/posts/2020-04-06/rss_junctions.gif){:class="img-fluid"}

<div class="alert alert-warning">
  <small><i><strong>Warning!</strong> SUMO Traffic lights will not be generated in the released CARLA maps. They were added manually, and cannot be retrieved from the OpenDRIVE.</i></small>
</div>